{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6498e8e2cbd342e50318858617c2abb8",
     "grade": false,
     "grade_id": "cell-c499aa2397f8b901",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\".\n",
    "\n",
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All). On JupyterLab, you may want to hit the \"Validate\" button as well.\n",
    "\n",
    "Caution: do not mess with the notebook's metadata; do not change a pre-existing cell's type; do not copy pre-existing cells (add new ones with the + button instead). This will break autograding; you will get a 0; you are warned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e1d9920a1618d90609e44321de6b10c4",
     "grade": false,
     "grade_id": "cell-1c5b73c4d79ae4a9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<table style=\"width: 100%; border: none;\" cellspacing=\"0\" cellpadding=\"0\" border=\"0\">\n",
    "  <tr>\n",
    "    <td><img src=\"https://www.ip-paris.fr/voeux2022-ecolepolytechnique/images/logo_p.png\" style=\"float: left; width: 100%\" />\n",
    "</td>\n",
    "    <td><a style=\"font-size: 3em; text-align: center; vertical-align: middle;\" href=\"https://moodle.polytechnique.fr/course/view.php?id=19260\">[CSC2S004EP - 2024] - Introduction to Machine Learning</a>\n",
    "</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "DH_UVBorem-E",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "52194f24ceb844d0abaf36dd71414d17",
     "grade": false,
     "grade_id": "cell-58fd8e246b985b3b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Lab Session 14: Reinforcement Learning\n",
    "\n",
    "Jesse Read and Adrien Ehrhardt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2a1d7431c9ea7de003746fb0a6ed5a85",
     "grade": false,
     "grade_id": "cell-14b09d24d1171884",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "be60f35cfa684b03b37e587d2f6d8d14",
     "grade": false,
     "grade_id": "cell-a9e7f1b348cd1a1d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Reinforcement Learning (RL) is not so exotic when we consider that it is the minimisation of expected loss, just like the rest of machine learning. However, in RL we often talk about rewards, and gain (or, return) being the sum of rewards; therefore we want to maximise this, in expectation. \n",
    "\n",
    "Consider an environment described by state $s_t \\in \\{0,1,2,3\\}$ at time $t$ (grid squares indexed from left to right), and action $a_t \\in \\{-1,0,+1\\}$ (for moving left, or staying in the same grid square, or right) and reward $r(3,+1) = 0$ ($s=3, a=1$; and $r(\\cdot,\\cdot) = -1$ for all other cases):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d0ea5fb61f8488c24664004e65f8dd3",
     "grade": false,
     "grade_id": "cell-c4b8d1af03090ed7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.states = [0, 1, 2, 3]\n",
    "        self.actions = [-1, 0, +1]\n",
    "        self.terminal_state = 3\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = 0\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        # Clip action to stay within valid actions\n",
    "        if action not in self.actions:\n",
    "            raise ValueError(f\"Invalid action {action}, must be one of {self.actions}\")\n",
    "        \n",
    "        # Compute next state\n",
    "        next_state = np.clip(self.state + action,min(self.states),max(self.states))\n",
    "\n",
    "        # Compute reward\n",
    "        if next_state == 3 and action == 1:\n",
    "            reward = 0\n",
    "            done = True\n",
    "        else:\n",
    "            reward = -1\n",
    "            done = False\n",
    "\n",
    "        self.state = next_state\n",
    "        return self.state, reward, done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2d0e685834bc95d1c8e16d21b6a9b05e",
     "grade": false,
     "grade_id": "cell-300c830d862cda6d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We need to interact with the environment, it means we need a *policy* to decide which action to take from which state. Let's start with a random one (what other choice do we have?). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a340e193dd27a2b2f92384303b91ef22",
     "grade": false,
     "grade_id": "cell-13daa27fcbadd61a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def policy(s):\n",
    "    return np.random.choice([-1,0,+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d22fad91193d2e350786af8ceaa4e6c5",
     "grade": false,
     "grade_id": "cell-8ec7132ac347848f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We take a **rollout** (or 'trajectory', 'episode'). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d85344073eb7f04f76601353b333ee00",
     "grade": false,
     "grade_id": "cell-c66e42dbfa7a5bfb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, -1)\n",
      "(0, -1, -1)\n",
      "(0, 0, -1)\n",
      "(0, 0, -1)\n",
      "(0, -1, -1)\n",
      "(0, 0, -1)\n",
      "(1, 1, -1)\n",
      "(0, -1, -1)\n",
      "(0, 0, -1)\n",
      "(1, 1, -1)\n",
      "(0, -1, -1)\n",
      "(0, -1, -1)\n",
      "(0, 0, -1)\n",
      "(1, 1, -1)\n",
      "(0, -1, -1)\n",
      "(0, 0, -1)\n",
      "(1, 1, -1)\n",
      "(1, 0, -1)\n",
      "(0, -1, -1)\n",
      "(0, -1, -1)\n",
      "(0, -1, -1)\n",
      "(0, -1, -1)\n",
      "(0, -1, -1)\n",
      "(1, 1, -1)\n",
      "(1, 0, -1)\n",
      "(1, 0, -1)\n",
      "(1, 0, -1)\n",
      "(1, 0, -1)\n",
      "(1, 0, -1)\n",
      "(1, 0, -1)\n",
      "(2, 1, -1)\n",
      "(2, 0, -1)\n",
      "(1, -1, -1)\n",
      "(1, 0, -1)\n",
      "(1, 0, -1)\n",
      "(1, 0, -1)\n",
      "(1, 0, -1)\n",
      "(2, 1, -1)\n",
      "(1, -1, -1)\n",
      "(0, -1, -1)\n",
      "(0, 0, -1)\n",
      "(0, -1, -1)\n",
      "(0, -1, -1)\n",
      "(0, -1, -1)\n",
      "(0, 0, -1)\n",
      "(0, -1, -1)\n",
      "(1, 1, -1)\n",
      "(1, 0, -1)\n",
      "(2, 1, -1)\n",
      "(1, -1, -1)\n",
      "(0, -1, -1)\n",
      "(0, 0, -1)\n",
      "(0, -1, -1)\n",
      "(0, -1, -1)\n",
      "(0, 0, -1)\n",
      "(1, 1, -1)\n",
      "(2, 1, -1)\n",
      "(3, 1, 0)\n"
     ]
    }
   ],
   "source": [
    "env = Environment()\n",
    "s = env.reset()\n",
    "done = False\n",
    "\n",
    "rollout = []\n",
    "\n",
    "while not done:\n",
    "    a = policy(s)\n",
    "    s, r, done = env.step(a)\n",
    "    print(f\"({s}, {a}, {r})\")\n",
    "    rollout.append((s,a,r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "68eaf290cbf486672e909f4ba28f963e",
     "grade": false,
     "grade_id": "cell-90c88dd02d6a19bb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "What can we *learn* from this? Complete the following code to estimate the **value** of taking action $a$ from state $s$, for all $(s,a)$ pairs that occur in the rollout. So we are estimating $Q(s,a) = \\mathbb{E}_{\\pi}[\\sum r_t \\mid s, a]$. It means: $Q(s,a)$ is the expected sum of rewards obtained from state $s$, taking action $a$, under policy $\\pi$ (the random one defined above).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a7d0e686e8cfc883bc7c2885e4e0bba9",
     "grade": false,
     "grade_id": "cell-3da5b4d15838fad4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Mapping for action <-> index\n",
    "actions = [-1, 0, 1]\n",
    "action_to_index = {a: i for i, a in enumerate(actions)}\n",
    "index_to_action = {i: a for a, i in action_to_index.items()}\n",
    "\n",
    "def estimate_q_from_rollout(rollout, num_states=4):\n",
    "    \"\"\"\n",
    "    Estimate Q(s,a) from a single rollout using first-visit Monte Carlo.\n",
    "    Returns a NumPy array of shape (num_states, num_actions).\n",
    "    \"\"\"\n",
    "    num_actions = len(actions)\n",
    "    returns = defaultdict(list)\n",
    "    visited = set()\n",
    "\n",
    "    for t in range(len(rollout)):\n",
    "        s, a, _ = rollout[t]\n",
    "        key = (s, a)\n",
    "        if key not in visited:\n",
    "            visited.add(key)\n",
    "            G = sum(r for _, _, r in rollout[t:])\n",
    "            returns[key].append(G)\n",
    "\n",
    "    Q = np.full((num_states, num_actions), np.nan)\n",
    "    for (s, a), G_list in returns.items():\n",
    "        a_idx = action_to_index[a]\n",
    "        Q[s, a_idx] = np.mean(G_list)\n",
    "\n",
    "    return Q\n",
    "\n",
    "def display_q_table(Q):\n",
    "    \"\"\"\n",
    "    Display Q-table.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(Q, columns=[f\"${a}$\" for a in actions])\n",
    "    df.index.name = \"State\"\n",
    "    return df.style.background_gradient(cmap=\"Greens\", axis=None).format(precision=1, na_rep=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "da9674621c4464845f7f88050d7cfe0d",
     "grade": false,
     "grade_id": "cell-3e3e634fa2755f13",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_26aac_row0_col0 {\n",
       "  background-color: #f5fbf2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_26aac_row0_col1 {\n",
       "  background-color: #f7fcf5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_26aac_row0_col2, #T_26aac_row2_col0, #T_26aac_row3_col0, #T_26aac_row3_col1 {\n",
       "  background-color: #000000;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_26aac_row1_col0 {\n",
       "  background-color: #5bb86a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_26aac_row1_col1 {\n",
       "  background-color: #b8e3b2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_26aac_row1_col2 {\n",
       "  background-color: #e8f6e4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_26aac_row2_col1 {\n",
       "  background-color: #62bb6d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_26aac_row2_col2 {\n",
       "  background-color: #6abf71;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_26aac_row3_col2 {\n",
       "  background-color: #00441b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_26aac\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_26aac_level0_col0\" class=\"col_heading level0 col0\" >$-1$</th>\n",
       "      <th id=\"T_26aac_level0_col1\" class=\"col_heading level0 col1\" >$0$</th>\n",
       "      <th id=\"T_26aac_level0_col2\" class=\"col_heading level0 col2\" >$1$</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >State</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_26aac_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_26aac_row0_col0\" class=\"data row0 col0\" >-56.0</td>\n",
       "      <td id=\"T_26aac_row0_col1\" class=\"data row0 col1\" >-57.0</td>\n",
       "      <td id=\"T_26aac_row0_col2\" class=\"data row0 col2\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_26aac_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_26aac_row1_col0\" class=\"data row1 col0\" >-25.0</td>\n",
       "      <td id=\"T_26aac_row1_col1\" class=\"data row1 col1\" >-40.0</td>\n",
       "      <td id=\"T_26aac_row1_col2\" class=\"data row1 col2\" >-51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_26aac_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_26aac_row2_col0\" class=\"data row2 col0\" ></td>\n",
       "      <td id=\"T_26aac_row2_col1\" class=\"data row2 col1\" >-26.0</td>\n",
       "      <td id=\"T_26aac_row2_col2\" class=\"data row2 col2\" >-27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_26aac_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_26aac_row3_col0\" class=\"data row3 col0\" ></td>\n",
       "      <td id=\"T_26aac_row3_col1\" class=\"data row3 col1\" ></td>\n",
       "      <td id=\"T_26aac_row3_col2\" class=\"data row3 col2\" >0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7ff004294380>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = estimate_q_from_rollout(rollout)\n",
    "display_q_table(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2bafe9d4a9e12459cc780016e71a7c0c",
     "grade": false,
     "grade_id": "cell-e224b31e2024c106",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Is that what you expect? If we act on *this* knowledge (current `Q`), is it better than random (i.e., better than not knowing anything / have we learned something from that rollout)?\n",
    "\n",
    "Your task is to take the first step of iteration in this RL algorithm, and rewrite the `policy` as the greedy policy (in the box below), i.e., the policy that takes the best action `a` in a given state `s` given its knowledge in `Q`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b5cfe4a59f7415f09b6f8bfe5de77bd6",
     "grade": false,
     "grade_id": "cell-890dc4dc6b7adc3e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def policy(s, Q):\n",
    "    \"\"\"\n",
    "    Greedy policy: returns the action a ∈ {-1, 0, 1} for given state s\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    q_values = Q[s]\n",
    "    max_q = np.nanmax(q_values)\n",
    "    best_actions = [a for a_idx, a in enumerate(actions) if q_values[a_idx] == max_q]\n",
    "    \n",
    "    if not best_actions:\n",
    "        return np.random.choice(actions)\n",
    "    \n",
    "    return np.random.choice(best_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fbd11ad336081afd9f87714998818519",
     "grade": false,
     "grade_id": "cell-66465421d38ed68a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You would find that if you generated a new rollout with *this* policy, then recalculated the greedy policy, you would **converge to the optimal policy**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "886380217e8d452a3532fd3d47401074",
     "grade": true,
     "grade_id": "cell-4a7e6cc6eafdf7c9",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4bd59d6b699b6600b5b338822f7e3fff",
     "grade": false,
     "grade_id": "cell-4678e0de767fdadd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Afterword: The goal of RL is such that our learned policy approaches (with sufficient training data) the optimal policy (which, in the real world, we do not have). This is essentially the same goal we have in any machine learning setting (except, we call it a policy $\\pi$ in RL, rather than a regression function $f$, or a classifer $h$ as in other paradigms). As elsewhere in machine learning, we invest our time to find models and algorithms which achieve better results, and more efficiently, or more explainably; but there is no single best model/algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
